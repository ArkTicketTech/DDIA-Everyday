## 分区再平衡

分布式系统，经常会面临这样的问题：

1. 节点因故障而下线，上线新的节点来做替换
2. 查询负载增加，需要新的节点来分担压力；
3. 存储数据增加，需要加入新的磁盘来存储

一旦发生这些问题，就需要把数据从当前节点转移到新上线的节点，这一过程被称为再平衡（rebalance）。

再平衡需要满足一些最低要求：

1. 再平衡后，负载（存储、查询、写入）在所有节点仍然是公平的；
2. 再平衡时，不能下线服务，数据库能够继续处理写入和查询；
3. 再平衡的速度应该尽可能快，如只移动必须的数据减少网络和磁盘开销

### 再平衡策略

之前讲的分区策略说了两种，基于key的hash分区和基于key的范围分区。

#### hash(key) mod N

基于key的hash分区说的是给每个分区分配一个hash范围，例如0 <= hash(key) < b0, b0 <= hash(key) < b1, ..

那hash(key)  对分区总数N取模 来决定分区，意思就是第i个分区的hash(key)范围为 i <= hash(key) < i + 1，相当于给每个分区分配了一个从0-N的一个数来作为代表。

这种分区策略很简单，但不健壮，取模意味着一旦分区总数N发生变化，几乎所有的数据的映射也会发生变化，如果按照这样的策略来，那再平衡的时候成本明显是最高昂的。

#### 固定数量分区

节点数与分区数并不是一定相等的，比如你有4个节点，但是你可以将你的一份数据分为5个分区，这样5个分区是分布在4个节点上，一定会有一个节点有两个分区的数据。

一般来说，有多少节点，就创建多少分区是最合适的，但是考虑到未来负载的增长和再平衡，我们可以把分区设置的大一点，比如现在4个节点，我们设置为5个分区，如果未来还要再加一个节点，那么通过再平衡这5个分区将分布在5个节点上。

5个分区，每个分区4个副本（包括主分区自己），共计5 * （3 + 1） = 20个分区副本

再平衡时，只需要从每一个节点拿一个副本到第5个节点即可实现平衡

一般是拿第5个主分区，和其他每个分区的副本

这个好处是 只有分区在节点之间移动，分区的数量不会变，key所指向的分区也不会变，改变的是分区所在的节点（还需要维护一份分区到节点的映射？）

缺点是提前预判的分区数量并不一定最符合未来增长，分区太多会增大管理开销。


#### 动态分区
动态分区是指相比于一开始就固定分区数量来说，分区的数量其实是可以根据数据量而变化的。key range和key hash的策略都支持动态分区。

对于key range的分区策略来说，数据在每个范围内并不一定都是均匀分布的，热点数据不能避免的话，一开始就固定了分区数量，自然对负载不友好。


key range分区策略下，一般都会支持动态分区，按生命周期来说：

1. 开始，数据量很少，只有一个分区。
2. 随着数据量不断增长，单个分区超过一定**上界**，则按尺寸一分为二，变成两个新的分区。
3. 如果某个分区，数据删除过多，少于某个**下界**，则会和相邻分区合并。

动态分区的优点是，分区数据适配于数据量。

#### 与节点比例分区

动态分区，分区的数量与数据集的大小成正比，但与节点的数量无关。

另一种策略是让分区数与节点数成正比（Cassandra），每个节点具有固定数量的分区，如果增加节点，分区数也将增加。

假设集群有 m 个节点，每个节点有 n 个分区，在此种均衡策略下，当有一个新节点加入时，会从 m*n 个分区中随机选择 n 个分区，将其一分为二，一半由新节点分走，另一半留在原机器上。

###  再平衡时机

全自动再平衡有一定的风险，因为再平衡是一个昂贵的操作，涉及到数据的迁移，可能会有较大的开销，如果在业务高峰期执行风险会比较大。假如一个节点过载，并且对请求的响应暂时很慢，其他节点得出结论，过载的节点已经死亡，并自动重平衡集群，使负载离开集群，这会对已经超负荷的节点、其他节点、网络造成更大的负载，时情况变得更糟糕。

因此一般的策略是 系统通过负载情况给出再平衡策略，由人工管理员审核没问题后，决定某个时间段运行（避开正常流量高峰），Couchbase、Riak 和 Voldemort 便采用了类似做法。
