#### 内存带宽和向量化

数仓的数据处理规模很大，因此会遇到两种瓶颈：

1. 内存处理带宽
2. CPU分支预测错误和流水线停顿

关于内存的瓶颈可已通过前述的数据压缩来缓解。对于 CPU 的瓶颈可以使用：

1. 列式存储和压缩可以让数据尽可能多地缓存在 L1 中，结合位图存储进行快速处理。
2. 使用 SIMD 用更少的时钟周期处理更多的数据。

#### 列存的tuple排序

列存数据库的行存储顺序不太重要，因为主要使用的聚合操作是：sum、avg、min和max，但也免不了需要对某些列利用条件进行筛选，为此我们可以如 LSM-Tree 一样，对所有行按某一列进行排序后存储。

> 注意，不可能同时对多列进行排序。因为我们需要维护多列间的下标间的对应关系，才可能按行取数据。

同时，排序后的那一列，压缩效果会更好。

因为数仓基本都是分布式系统，并且有多副本机制，那么多副本可以按照不同的方式排序，以满足不同的查询需求。当然，这样也最多只能建立副本数（通常是 3 个左右）列索引。

这一想法由 C-Store 引入，并且为商业数据仓库 Vertica 采用。

#### 列存的写入

以上优化都是针对数仓的经典应用场景——读多写少，当然这个少是指频率，写入的数据可真不少。

> 面向列的存储 、 压缩和排序都非常有助千加速读取查询。 但是, 它们的缺点是让写入更加困难。

比如 B 树的**原地更新流**是不太行的。举个例子，要在中间某行插入一个数据，**纵向**来说，会影响所有的列文件（如果不做 segment 的话）；为了保证多列间按下标对应，**横向**来说，又得更新该行不同列的所有列文件。

所幸我们有 LSM-Tree 的追加流。

1. 将新写入的数据在**内存**中 Batch 好，按行按列，选什么数据结构可以看需求。
2. 然后达到一定阈值后，批量刷到**外存**，并与老数据合并。

数仓 Vertica 就是这么做的。
