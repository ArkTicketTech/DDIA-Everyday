### 实践中的分布式事务

分布式事务的使用不是常见的。

分布式事务的某些实现会带来比较严重的性能损失，比如MySQL的分布式事务比单节点事务慢10倍以上。

分布式事务的类型：

1. 数据库内部的分布式事务：

1. 1. 一些分布式数据库支持数据库节点之间的内部事务，所有参与事务的节点都运行相同的数据库软件；

1. 异构分布式事务：

1. 1. 参与者是由两种或两种以上的不同技术组成的，例如来自不同供应商的两个数据库，甚至是非数据库系统（如消息代理）

数据库内部事务不必与任何其他系统兼容，因此他们可以使用任何协议，并能针对特定技术进行指定的优化。



#### 恰好一次的消息处理

异构的分布式事务也有它的应用场景，比如消息队列中的一条消息可以被确认为已提交，当且仅当用于处理消息的数据库事务成功提交。

这是通过在同一个事务中原子提交「消息确认」和「数据库写入」两个操作来实现的。

两个动作要么都成功，要么都中止。即使在成功之前需要几次重试，也可以确保消息有效地被恰好处理一次。

然而，重试必须保证没有副作用，假设每次重试都需要发送邮件，那么多次重试就会发送多次邮件，这样业务就会很奇怪。



#### XA事务

X/Open X（扩展架构eXtended Architecture的缩写）是跨异构技术实现两阶段提交的标准。它于1991年推出并实现了广泛的实现，许多传统关系数据库（包括PostgreSQL、MySQL等）和消息队列（ActiveMQ）都支持XA。

XA不是一个网络协议，它只是一个用来与事务协调者连接的C API。其他语言也有这种API都绑定；例如在Java EE应用的世界中，XA事务是使用Java事务API（JTA， Java Transaction API）实现的，而许多使用JDBC的数据库驱动，以及许多使用JMS的消息代理都支持JTA。

XA假定你的应用使用网络驱动或客户端来与参与者（数据库或消息服务）进行通信。如果驱动支持XA，则意味着它会调用XA API以查明操作是否为分布式事务的一部份，如果是，则将必要的信息发送数据库服务器。驱动还会向协调者暴露回调接口，协调者可以通过回调来要求参与者准备、提交或中止。



事务协调者需要实现XA API，标准没有指明应该如何实现，但实际上协调者通常只是一个库，被加载到发起事务的应用的同一个进程中（而不是单独的服务）。它在事务中跟踪所有的参与者，并在要求它们准备之后收集参与者的响应（通过驱动回调），并使用本地磁盘的日志记录每次事务的决定。

如果应用程序崩溃，或者运行应用的机器报销了，协调者也挂了。然后任何带有准备了但未交的事务的参与者都会在疑虑中卡死。由于协调者的日志位于应用服务器的本地磁盘上，因此必须重启该服务器，且协调程序库必须读取日志以恢复每个事务的结果。只有这样，协调者才能使用数据库驱动的XA回调来要求参与者提交或中止。数据库服务器不能直接联系协调者，因为所有通信必须通过客户端。



#### 怀疑时持有锁

存疑事务的危害：读已提交的弱隔离级别中，数据库事务通常获取待修改的行上的行锁，以防止脏写；可串行化，使用两阶段锁定的数据库也必须为事务所读取的行加上共享锁。

在事务提交或中止之前，数据库不能释放这些锁，因此2PC中的事务必须在存疑期间持有这些锁。如果协调者崩溃了20分钟，那么这些锁就会被持有20分钟。存疑事务有可能导致应用大面积进入不可用状态，直到存疑事务被解决。



#### 协调者从故障中恢复

理论上，如果协调者崩溃并重新启动，它应该干净地从日志中恢复其状态，并解决任何存疑事务。然而在实践中，孤立的存疑事务确实会出现，即无论处于何种理由，协调者无法确定事务的结果（例如事务日志已经由于软件错误丢失或损坏）。这些事务无法自动解决，所以它们永远待在数据库中，持有锁并阻塞其他事务。

如果事务日志丢失或者损失，难道不能当作没有记录，而进行事务的中止吗？不明白

即使重启数据库服务器也无法解决这个问题，因为在2PC的正确实现中，即使重启也必须保留存疑事务的锁（否则就会冒违反原子性保证的风险）。

唯一的出路还是要靠管理员手动决定提交还是回滚，但这太不合理了。

许多XA的实现都有一个启发式决策的紧急逃生舱口，允许参与者单方面决定放弃或者提交一个存疑事务，而无需协调者做出最终决定。但本质上也是破坏了原子性，它违背了2PC的系统承诺。因此，启发式决策是为了逃出灾难性的情况而准备的，而不是日常使用。 

#### 分布式事务的限制

事务协调者本身就是一种数据库（存储了事务的结果）。

1. 如果协调者是单个节点，没有复制，那么它将是整个系统的失效单点
2. 许多服务端都是无状态的，可以任意水平扩展，当协调者本身成为应用服务器的一部份时，它会使得服务成为有状态的，改变了部署的性质。因为协调者的日志成为持久化系统状态的关键部分，与数据库本身一样重要，因为协调者日志是为了在崩溃后恢复存疑事务所必须的；
3. 由于XA需要和足够广泛的数据系统进行适配，因此其API只能维持一个最小公共接口集，由此带来了XA在能力上的诸多限制。XA不能跨系统检测死锁，因为这要求增加一种可以获取所有正在等待的锁信息接口；XA也不能提供跨系统的SSI隔离级别，因为这要求支持一种跨系统检测冲突的协议。
4. 2PC必须要所有的参与者都回复，因此一旦系统内任何子模块损坏，事务也随之消息。分布式事务有扩大失效的趋势。



更好的实现在第十一章和十二章讲到。

### 容错共识

共识意味着让多个节点就某事达成共识。

例如：如果有多个人同时尝试预订飞机上的最后一个座位，共识算法可以用来确定这些互不相容的操作中，哪一个才是赢家。

共识问题通常形式化如下：一个或多个节点可以提议某些值，而共识算法决定采用其中的某个值。

共识算法必须满足以下性质：

1. 一致同意：没有两个节点的决定不同
2. 完整性：没有节点决定两次
3. 有效性：如果有一个节点决定了v，则v由某个节点所提议
4. 终止：由所有未崩溃的节点来最终决定值

如果不关注容错，前三个属性就很容易满足：你可以将一个节点硬编码为独裁者，并让该节点做出所有的决定。但如果该节点失效，那么系统就无法再作出任何决定。



终止属性形式化了容错的思想，即使部分节点出现故障，其他节点也必须达成一致决定。

共识的系统模型假设，任何需要等待节点恢复的算法都不能满足终止属性，特别是2PC就不符合终止属性的要求。



但节点崩溃也是有限制的，至少占总体多数的节点没有崩溃，多数就可以安全地组成法定人数。

不讨论拜占庭式的错误。



#### 共识算法与全序广播

最著名的容错共识算法是「视图戳复制」、Paxos、Raft以及Zab。

这些算法实际上并不直接使用这里描述的形式化模型，而是决定值的顺序，这迫使它们成为全序广播算法。

全序广播算法要求将消息按照相同的顺序，恰好传递一次，准确传送到所有节点。如果仔细考虑，这相当于进行了几轮共识：在每一轮中，节点提议下一条要传送的消息，然后决定在全序中下一条要发送的消息。

所以，全序广播相当于重复进行多轮共识（每轮共识决定与一次消息传递相对应）：

1. 由于一致同意属性，所有节点以相同的顺序传递相同的消息
2. 由于完整性属性，消息不会重复
3. 由于有效性属性，消息不会被损坏，也不能凭空编造
4. 由于终止属性，消息不会丢失

「视图戳复制」、Multi-Paxos、Raft以及Zab直接实现了全序广播。

#### 单主复制与共识

手动配置的主库，单主复制实际上就是一个全序广播。

但自动选举领导者的数据库，

1. 需要共识算法来选择领导者，共识算法等价于全序广播，再等价于单主复制。
2. 而单主复制需要一个领导者

这里就循环依赖了，如何打破这个依赖，需要下面的纪元编号与法定人数。

#### 纪元编号与法定人数

迄今为止所讨论的所有共识协议，在内部都以某种形式使用一个领导者，但它们并不能保证领导者是独一无二的。相反，它们在协议中用一个纪元编号（epoch number，Paxos中称为ballot number、视图戳复制中称为view number、raft中称为term number），并确保在每个时代中，领导者都是唯一的。

每次当现任领导被认为挂掉的时候，节点间就会开始一场投票，以选出一个新领导。这次选取被赋予一个递增的纪元编号，因此纪元编号是全序且单调递增的。如果两个不同时代的领导者出现冲突，那么带有更高纪元编号的领导者说了算。

在任何领导者被允许决定任何事情之前，必须先检查是否存在其他带有更高纪元编号的领导者，它们可能会做出相互冲突的决定。那一个领导者如何知道自己没有被其他领导者赶下台，注意一个领导者认为自己是领导者，不意味着其他节点也认可这一点。

所以领导者在决策前需要首先先从所有节点获取法定票数，然后对于每个决策，主节点都必须将其作为提案发给其他所有节点，并且等待法定节点的同意。如果法定节点的回复中没有任何更高纪元的，则当前没有发生过更高时代的领导选举。

如果发生了新的选举，新的领导者一定在法定节点中。



#### 共识的局限性

共识算法对于分布式系统来说是一个巨大的突破

1. 它为其他充满不确定的系统带来了基础的安全属性（一致同意、完整性、有效性）
2. 还可以保持容错：它们提供了全序广播，因此他们也可以以一种容错的方式实现线性一致的原子操作。



但共识算法的应用也是有代价的

1. 同步复制损失性能：每次进行决策前都要让多数节点进行投票
2. 需要严格多数来运转：容忍单点故障需要三个节点，容忍两个节点故障需要5个节点；如果网络故障切断了某些节点同其他节点的连接，则只有多数节点所在的网络可以继续工作，其余将阻塞。
3. 共识算法的动态成员扩展比静态成员难理解；
4. 复杂网络环境性能更差：共识系统通常使用超时机制来对故障节点进行检测，网络抖动会导致频繁的领导者选举，系统性能会变差



设计能健壮应对不可靠网络的算法仍然是一个开放的研究问题。
